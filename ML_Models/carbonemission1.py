# -*- coding: utf-8 -*-
"""carbonemission1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13z-FG8Sqy7le8cWF13m2sWfqCbmAxQ5i
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
# TODO: Replace the following placeholder path with the actual path to your carbon.csv file in Google Drive.
file_path = '/content/drive/My Drive/data/Carbon.csv'
df = pd.read_csv(file_path)
print(df.head())

# getting dataframe shape
df.shape

# getting stats of numerical data
df.describe()

# renaming columns
df.columns = ['Body Type', 'Sex', 'Diet', 'Shower', 'Heating', 'Transport', 'Vehicle', 'Social', 'Grocery', 'Flight', 'Vehicle Distance',
              'Bag Size', 'Waste Weekly', 'TV Daily Hour', 'Clothes Monthly', 'Internet Daily', 'Energy Eff', 'Recycling', 'Cooking',
              'CarbonEmission']
df.head()

"""Data cleaning

"""

# checking for null values
df.isna().sum()

import numpy as np
df.replace(np.nan, 'None', inplace=True)
df.head()

"""Feature Engineering"""

import ast

df['Recycling'] = df['Recycling'].apply(ast.literal_eval)
recycling_items = list(set(item for sublist in df['Recycling'] for item in sublist))

df['Cooking'] = df['Cooking'].apply(ast.literal_eval)
cooking_items = list(set(item for sublist in df['Cooking'] for item in sublist))

# Create columns for recycling
for item in recycling_items:
    df[item] = df['Recycling'].apply(lambda x: 1 if item in x else 0)

# Create columns for cooking
for item in cooking_items:
    df[item] = df['Cooking'].apply(lambda x: 1 if item in x else 0)

df = df.drop(columns=['Recycling', 'Cooking'])
df.head()

# restructuring features
df = df[['Body Type', 'Sex', 'Diet', 'Shower', 'Heating', 'Transport', 'Vehicle', 'Social', 'Grocery', 'Flight', 'Vehicle Distance', 'Bag Size',
         'Waste Weekly', 'TV Daily Hour', 'Clothes Monthly', 'Internet Daily', 'Energy Eff', 'Plastic', 'Glass', 'Metal', 'Paper', 'Microwave',
         'Oven', 'Stove', 'Airfryer', 'Grill', 'CarbonEmission']]

df.head()

df.head(1).T

# defining numerical and categorical columns
cat_col = ['Body Type', 'Sex', 'Diet', 'Shower', 'Heating', 'Transport', 'Vehicle', 'Social','Flight', 'Bag Size',
           'Energy Eff', 'Plastic', 'Glass', 'Metal', 'Paper', 'Microwave', 'Oven', 'Stove', 'Airfryer', 'Grill']
num_col = ['Grocery', 'Vehicle Distance', 'Waste Weekly', 'TV Daily Hour', 'Internet Daily', 'Clothes Monthly']

"""Data Preprocessing"""

from sklearn.preprocessing import LabelEncoder

encoders = {}

for col in cat_col:
    encoder = LabelEncoder()
    df[col] = encoder.fit_transform(df[col])
    encoders[col] = encoder

df.head()

from sklearn.preprocessing import StandardScaler

scalers = {}

for column in num_col:
    scaler = StandardScaler()
    df[column] = scaler.fit_transform(df[[column]])
    scalers[column] = scaler

df.head()

# selecting features and target
X = df.drop('CarbonEmission', axis=1)
y = df['CarbonEmission']

# splitting data into train and test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""XGBoost Regressor"""

import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print(f'XGBoost Regressor Results:')
print(f'Mean Absolute Error: {mae_xgb}')
print(f'Mean Squared Error: {mse_xgb}')
print(f'Root Mean Squared Error: {rmse_xgb}')
print(f'R-squared: {r2_xgb}')

"""Gradient Boosting Regressor"""

from sklearn.ensemble import GradientBoostingRegressor

gbr_model = GradientBoostingRegressor()
gbr_model.fit(X_train, y_train)
y_pred_gbr = gbr_model.predict(X_test)

mae_gbr = mean_absolute_error(y_test, y_pred_gbr)
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
rmse_gbr = np.sqrt(mse_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f'Gradient Boosting Regressor Results:')
print(f'Mean Absolute Error: {mae_gbr}')
print(f'Mean Squared Error: {mse_gbr}')
print(f'Root Mean Squared Error: {rmse_gbr}')
print(f'R-squared: {r2_gbr}')

"""Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

rfr_model = RandomForestRegressor()
rfr_model.fit(X_train, y_train)
y_pred = rfr_model.predict(X_test)

mae_rfr = mean_absolute_error(y_test, y_pred)
mse_rfr = mean_squared_error(y_test, y_pred)
rmse_rfr = np.sqrt(mse_rfr)
r2_rfr = r2_score(y_test, y_pred)

print(f'Mean Absolute Error: {mae_rfr}')
print(f'Mean Squared Error: {mse_rfr}')
print(f'Root Mean Squared Error: {rmse_rfr}')
print(f'R-squared: {r2_rfr}')

"""Cat Boost Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install catboost

import catboost as cb

catboost_model = cb.CatBoostRegressor(
    iterations=1000,
    learning_rate=0.1,
    depth=5,
    eval_metric='RMSE',
    cat_features=[0, 1, 2, 3, 4, 5, 6],
    verbose=100
)

catboost_model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100)
y_pred_catboost = catboost_model.predict(X_test)

mae_catboost = mean_absolute_error(y_test, y_pred_catboost)
mse_catboost = mean_squared_error(y_test, y_pred_catboost)
rmse_catboost = np.sqrt(mse_catboost)
r2_catboost = r2_score(y_test, y_pred_catboost)

print(f'CatBoost Regressor Results:')
print(f'Mean Absolute Error: {mae_catboost}')
print(f'Mean Squared Error: {mse_catboost}')
print(f'Root Mean Squared Error: {rmse_catboost}')
print(f'R-squared: {r2_catboost}')

model_r2 = {
    'XG Boost Regression' : r2_xgb,
    'Catboost Regression' : r2_catboost,
    'Gradient Boost Regression' : r2_gbr,
    'Random Forest Regression' : r2_rfr,
}

model_rmse = {
    'XG Boost Regression' : rmse_xgb,
    'Catboost Regression' : rmse_catboost,
    'Gradient Boost Regression' : rmse_gbr,
    'Random Forest Regression' : rmse_rfr,
}

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))

# Plot MAE
bars_r2 = ax1.barh(list(model_r2.keys()), list(model_r2.values()), color='skyblue')
ax1.set_ylabel('Models', fontweight='bold')
ax1.set_xlabel('R-Squared Value', fontweight='bold')
ax1.set_title('R-Squared Value of ML Models', fontweight='bold')
ax1.set_xlim(0.9, 1.02)
for bar in bars_r2:
    width = bar.get_width()
    ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, '{:.5f}'.format(round(width, 5)), ha='left', va='center')

# Plot RMSE
bars_rmse = ax2.barh(list(model_rmse.keys()), list(model_rmse.values()), color='lightcoral')
ax2.set_ylabel('Models', fontweight='bold')
ax2.set_xlabel('Root Mean Squared Error', fontweight='bold')
ax2.set_title('Root Mean Squared Error of ML Models', fontweight='bold')
ax2.set_xlim(95, 375)
for bar in bars_rmse:
    width = bar.get_width()
    ax2.text(width + 5, bar.get_y() + bar.get_height()/2, '{:.5f}'.format(round(width, 5)), ha='left', va='center')

plt.tight_layout()
plt.show()

"""Predicting"""

new_data = {
    'Body Type': 'overweight',
    'Sex': 'female',
    'Diet': 'vegetarian',
    'Shower': 'daily',
    'Heating': 'coal',
    'Transport': 'public',
    'Vehicle': 'None',
    'Social': 'often',
    'Grocery': 200,
    'Flight': 'rarely',
    'Vehicle Distance': 500,
    'Bag Size': 'medium',
    'Waste Weekly': 3,
    'TV Daily Hour': 5,
    'Clothes Monthly': 10,
    'Internet Daily': 8,
    'Energy Eff': 'Yes',
    'Plastic': 1,
    'Glass': 0,
    'Metal': 1,
    'Paper': 0,
    'Microwave': 1,
    'Oven': 0,
    'Stove': 1,
    'Airfryer': 0,
    'Grill': 0
}

new_df = pd.DataFrame([new_data])

# Encode categorical features
for col in cat_col:
    new_df[col] = encoders[col].transform(new_df[col])

# Scale numeric features
for col in num_col:
    new_df[col] = scalers[col].transform(new_df[[col]])

prediction = xgb_model.predict(new_df)
print("Predicted Carbon Emission:", prediction[0])

print("CatBoost:", catboost_model.predict(new_df)[0])
print("Gradient Boosting:", gbr_model.predict(new_df)[0])
print("Random Forest:", rfr_model.predict(new_df)[0])

